マルコフ決定過程
章
2
キーワード
マルコフ性, MDP, 状態価値関数
更新日
ユーザー
押谷俊貴
ここで扱う問題は非定常問題である。エージェントの行動によって状況が変わる問題。
マルコフ決定過程（MDP）
（概要）
エージェントが環境と相互作用しながら行動を決定する過程。
非定常問題なので、行動によってエージェントの置かれる状況が変わっていく→
状態という。
最初の状態を
﻿、最初の行動を
﻿、最初の報酬を
﻿とすると、以下のように遷移する。
最初の状態
﻿から最初の行動
﻿を行うことで、最初の報酬
﻿を獲得し、次の状態
﻿に移
る。
時刻tを用いて、状態
﻿から行動
﻿を行うことで、報酬
﻿を獲得し、次の状態
﻿に移る。
これらを数式にしたい。
（数式）
定式化するにあたって、以下の３つの要素を数式で表す必要がある。
状態遷移：状態はどのように遷移するか
報酬：報酬はどのように与えられるか
方策：エージェントはどのように行動を決定するか
1. 状態遷移
次の状態を
﻿、今の状態を﻿、今の行動を﻿とすると、以下の式で状態遷移を表せられる。
@2025年6月23日 16:40
S
​0
A
​0
R
​0
S
​, A
​, R
​, S
​, A
​, R
​, S
​, ...
0
0
0
1
1
1
2
S
​0
A
​0
R
​0
S
​1
S
​t
A
​t
R
​t
S
​
t+1
s’
s
a
P(s ∣
′
s, a)
マルコフ決定過程
1
上の式は、状態と行動が与えられた時に状態
﻿に遷移する確率ということ。この確率を状態
遷移確率という。
この時、次の状態は現在の状態と現在の行動だけに依存している。それより前の状態や行動
は関係ない。→ マルコフ性
このマルコフ性を仮定することによって、問題を解きやすくしている
2. 報酬関数
この本では報酬を決定論的に与えられる想定で進めている。
報酬
﻿は現在の状態と行動と次の状態が与えられたら、一意に決まる。
3. エージェントの方策
エージェントは「現在の状態」だけに基づいて行動を決められる。
→ 環境がマルコフ性を有しているから
なぜ環境がマルコフ性を有していると「現在の状態」だけに基づいて行動を決められるのか
→ 環境の遷移には過去の行動などは参照されない。つまり「現在の状態」が、それまでの過
去の状態・行動の情報を要約した十分な情報を持っているとみなせる。
ある状態
﻿にいる時に行動﻿をとる確率を表す。
（目標）
MDPの目標は状態遷移確率、報酬関数、エージェントの方策の枠組みの中で最適方策を見つ
けること。最適方策とは、収益が最大となる方策。
収益
時刻﻿で状態が
﻿である場合の時、方策﻿によって行動
﻿を行い、報酬
を得て、新しい状態
﻿に遷移する流れが続くとすると、収益
は以下のようにエージェントが得る報酬の和と
して表せる。
ここで﻿は割引率といい、時間が進むにつれて指数関数的に報酬が減衰される。こうすること
で、近い将来の報酬ほど重要に見せることができる。また、連続タスクを想定したとき、収益
が無限大になることを防ぐことができる。
s’
r(s, a, s )
′
R
π(a ∣s)
s
a
t
S
​t
π
A
​t
Rt
S
​
t+1
Gt
G
​ =
t
R
​ +
t
γR
​ +
t+1
γ R
​ +
2
t+2
γ R
​ +
3
t+3
...
γ
マルコフ決定過程
2
エピソードタスクと連続タスク
エピソードタスクは終わりがある問題。たとえば、囲碁などは最終的に「勝ち・負け・引き
分け」に行き着くので、エピソードタスクになる。DonkeyCarもエピソードタスク。コース
アウトした、一定時間が経過した、目標周回回数に達したのように終わりがある。
連続タスクは、在庫管理を行う問題など永遠に続くような「終わり」を作らない問題。
この「収益」を最大化することがエージェントの目標だが一つ問題がある。方策と状態遷移
は確率なので、得られる収益は確率的な振る舞いをする。あるエピソードでは収益が10、あ
るエピソードでは収益が8.3など確率的に変動する。これに対応するため、状態価値関数を使
用する。
状態価値関数
確率的な挙動に対応するには期待値を使用する。ここでは収益の期待値を使う。
状態﻿で方策が﻿の時の
﻿の期待値。これを状態価値関数という。
一つの方策に対して、状態の数だけ値が存在する。
最適方策と最適状態価値関数
ある方策が、全ての状態において、他のどの方策よりも状態価値関数の値が大きい時、その
方策が最適方策
﻿となる。
MDPでは最適方策が少なくとも一つは存在する！しかも、決定論的方策になる（各状態にお
ける行動が一意に決まる）。
最適方策における状態価値関数は最適状態価値関数
﻿という。
v
​(s) =
π
E[G
​ ∣
t
S
​ =
t
s, π]
s
π
G
​t
π
​∗
v
​∗
マルコフ決定過程
3
